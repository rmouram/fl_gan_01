{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from threading import Thread\n",
    "from threading import Lock\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 5\n",
    "local_epoch = 10\n",
    "users = 2 # number of clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romulo/code/Federated-Learning-and-Split-Learning-with-raspberry-pi/env/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romulo/code/Federated-Learning-and-Split-Learning-with-raspberry-pi/env/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generator = tf.keras.models.load_model('gan_model_gen')\n",
    "discriminator = tf.keras.models.load_model('gan_model_dis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Função para plotar imagens geradas\n",
    "# def plot_generated_images(epoch, generator, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
    "#     noise = np.random.normal(0, 1, size=[examples, 100])\n",
    "#     generated_images = generator.predict(noise)\n",
    "#     generated_images = generated_images.reshape(examples, 28, 28)\n",
    "\n",
    "#     plt.figure(figsize=figsize)\n",
    "#     for i in range(generated_images.shape[0]):\n",
    "#         plt.subplot(dim[0], dim[1], i + 1)\n",
    "#         plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
    "#         plt.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'gan_generated_image_epoch_{epoch}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_generated_images(10, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Função para fazer previsões (discriminar)\n",
    "# def predict_images(discriminator, images):\n",
    "#     # Normalizar as imagens para o intervalo [-1, 1]\n",
    "#     images = (images - 127.5) / 127.5\n",
    "    \n",
    "#     # Achatando as imagens para vetores\n",
    "#     images_flat = images.reshape(images.shape[0], -1)\n",
    "    \n",
    "#     # Fazer previsões\n",
    "#     predictions = discriminator.predict(images_flat)\n",
    "    \n",
    "#     return predictions\n",
    "\n",
    "# # Carregar imagens reais do MNIST para teste (ou substituir por suas próprias imagens)\n",
    "# (train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# # Selecionar um subconjunto de imagens para teste\n",
    "# num_images_to_test = 10\n",
    "# test_images = train_images[:num_images_to_test]\n",
    "\n",
    "# # Adapte as dimensões das imagens e adicione a dimensão do canal\n",
    "# test_images = test_images.reshape(num_images_to_test, 28, 28, 1).astype('float32')\n",
    "\n",
    "# # Fazer previsões usando o discriminador\n",
    "# predictions = predict_images(discriminator, test_images)\n",
    "\n",
    "# # Exibir as imagens e suas previsões\n",
    "# plt.figure(figsize=(26, 4))\n",
    "# for i in range(num_images_to_test):\n",
    "#     plt.subplot(2, num_images_to_test, i + 1)\n",
    "#     plt.imshow(test_images[i, :, :, 0], cmap='gray')\n",
    "#     plt.title(\"Real\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.subplot(2, num_images_to_test, i + 1 + num_images_to_test)\n",
    "#     plt.bar(range(1), predictions[i, 0], tick_label=['Fake'])\n",
    "#     plt.title(\"Discriminator Prediction\")\n",
    "#     plt.ylim([0, 1])\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsoclist = [0]*users\n",
    "\n",
    "start_time = 0\n",
    "weight_count = 0\n",
    "\n",
    "global_weights = copy.deepcopy(generator.get_weights())\n",
    "\n",
    "datasetsize = [0]*users\n",
    "weights_list = [0]*users\n",
    "\n",
    "lock = Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comunication overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sendsize_list = []\n",
    "total_receivesize_list = []\n",
    "\n",
    "client_sendsize_list = [[] for i in range(users)]\n",
    "client_receivesize_list = [[] for i in range(users)]\n",
    "\n",
    "train_sendsize_list = [] \n",
    "train_receivesize_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required socket functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    l_send = len(msg)\n",
    "    msg = struct.pack('>I', l_send) + msg\n",
    "    sock.sendall(msg)\n",
    "    return l_send\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg, msglen\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w, datasize):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scale each weight by its corresponding data size\n",
    "    for i, data in enumerate(datasize):\n",
    "        for j in range(len(w[i])):\n",
    "            w[i][j] = tf.multiply(w[i][j], float(data))\n",
    "    \n",
    "    # Create a deep copy of the first set of weights\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "\n",
    "    # Sum the scaled weights for each layer\n",
    "    for j in range(len(w_avg)):\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[j] = tf.add(w_avg[j], w[i][j])\n",
    "        \n",
    "        # Calculate the average by dividing the sum by the total data size\n",
    "        w_avg[j] = tf.divide(w_avg[j], float(sum(datasize)))\n",
    "\n",
    "    return w_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive users before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_thread(func, num_user):\n",
    "    global clientsoclist\n",
    "    global start_time\n",
    "    \n",
    "    thrs = []\n",
    "    for i in range(num_user):\n",
    "        conn, addr = s.accept()\n",
    "        print('Conntected with', addr)\n",
    "        # append client socket on list\n",
    "        clientsoclist[i] = conn\n",
    "        args = (i, num_user, conn)\n",
    "        thread = Thread(target=func, args=args)\n",
    "        thrs.append(thread)\n",
    "        thread.start()\n",
    "    print(\"timmer start!\")\n",
    "    start_time = time.time()    # store start time\n",
    "    for thread in thrs:\n",
    "        thread.join()\n",
    "    end_time = time.time()  # store end time\n",
    "    print(\"TrainingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive(userid, num_users, conn): #thread for receive clients\n",
    "    global weight_count\n",
    "    \n",
    "    global datasetsize\n",
    "\n",
    "\n",
    "    msg = {\n",
    "        'rounds': rounds,\n",
    "        'client_id': userid,\n",
    "        'local_epoch': local_epoch\n",
    "    }\n",
    "\n",
    "    datasize = send_msg(conn, msg)    #send epoch\n",
    "    total_sendsize_list.append(datasize)\n",
    "    client_sendsize_list[userid].append(datasize)\n",
    "\n",
    "    train_dataset_size, datasize = recv_msg(conn)    # get total_batch of train dataset\n",
    "    total_receivesize_list.append(datasize)\n",
    "    client_receivesize_list[userid].append(datasize)\n",
    "    \n",
    "    \n",
    "    with lock:\n",
    "        datasetsize[userid] = train_dataset_size\n",
    "        weight_count += 1\n",
    "    \n",
    "    train(userid, train_dataset_size, num_users, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(userid, train_dataset_size, num_users, client_conn):\n",
    "    global weights_list\n",
    "    global global_weights\n",
    "    global weight_count\n",
    "    global val_acc\n",
    "    \n",
    "    for r in range(rounds):\n",
    "        with lock:\n",
    "            if weight_count == num_users:\n",
    "                for i, conn in enumerate(clientsoclist):\n",
    "                    datasize = send_msg(conn, global_weights)\n",
    "                    total_sendsize_list.append(datasize)\n",
    "                    client_sendsize_list[i].append(datasize)\n",
    "                    train_sendsize_list.append(datasize)\n",
    "                    weight_count = 0\n",
    "\n",
    "        client_weights, datasize = recv_msg(client_conn)\n",
    "        total_receivesize_list.append(datasize)\n",
    "        client_receivesize_list[userid].append(datasize)\n",
    "        train_receivesize_list.append(datasize)\n",
    "\n",
    "        weights_list[userid] = client_weights\n",
    "        print(\"User\" + str(userid) + \"'s Round \" + str(r + 1) +  \" is done\")\n",
    "        with lock:\n",
    "            weight_count += 1\n",
    "            if weight_count == num_users:\n",
    "                #average\n",
    "                global_weights = average_weights(weights_list, datasetsize)\n",
    "                #train_discriminator(epochs, batch_size)\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinar apenas o Discriminador da GAN\n",
    "def train_discriminator(epochs=1, batch_size=128):\n",
    "    batch_count = x_train.shape[0] // batch_size\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for _ in range(batch_count):\n",
    "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
    "            generated_images = generator.predict(noise)\n",
    "\n",
    "            X = np.concatenate([image_batch, generated_images])\n",
    "            y_dis = np.zeros(2 * batch_size)\n",
    "            y_dis[:batch_size] = 0.9  # Rótulos suavizados para o treinamento estável\n",
    "\n",
    "            # Treina o discriminador\n",
    "            d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "        print(f'Época {e+1}/{epochs}, Discriminador Loss: {d_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.1.1\n"
     ]
    }
   ],
   "source": [
    "host = socket.gethostbyname(socket.gethostname())\n",
    "port = 10080\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket()\n",
    "s.bind((host, port))\n",
    "s.listen(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conntected with ('127.0.0.1', 57266)\n",
      "Conntected with ('127.0.0.1', 60172)\n",
      "timmer start!\n",
      "User1's Round 1 is done\n",
      "User0's Round 1 is done\n",
      "User1's Round 2 is done\n",
      "User0's Round 2 is done\n",
      "User1's Round 3 is done\n",
      "User0's Round 3 is done\n",
      "User1's Round 4 is done\n",
      "User0's Round 4 is done\n",
      "User1's Round 5 is done\n",
      "User0's Round 5 is done\n",
      "TrainingTime: 315.41452407836914 sec\n"
     ]
    }
   ],
   "source": [
    "run_thread(receive, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingTime: 315.43290734291077 sec\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()  # store end time\n",
    "print(\"TrainingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print all of communication overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---total_sendsize_list---\n",
      "total_sendsize size: 59750786 bytes\n",
      "\n",
      "\n",
      "---total_receivesize_list---\n",
      "total receive sizes: 59749310 bytes\n",
      "\n",
      "\n",
      "---client_sendsize_list(user0)---\n",
      "total client_sendsizes(user0): 29875393 bytes\n",
      "\n",
      "\n",
      "---client_receivesize_list(user0)---\n",
      "total client_receive sizes(user0): 29874655 bytes\n",
      "\n",
      "\n",
      "---client_sendsize_list(user1)---\n",
      "total client_sendsizes(user1): 29875393 bytes\n",
      "\n",
      "\n",
      "---client_receivesize_list(user1)---\n",
      "total client_receive sizes(user1): 29874655 bytes\n",
      "\n",
      "\n",
      "---train_sendsize_list---\n",
      "total train_sendsizes: 59750672 bytes\n",
      "\n",
      "\n",
      "---train_receivesize_list---\n",
      "total train_receivesizes: 59749280 bytes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('val_acc list')\n",
    "# for acc in val_acc:\n",
    "#     print(acc)\n",
    "\n",
    "print('\\n')\n",
    "print('---total_sendsize_list---')\n",
    "total_size = 0\n",
    "for size in total_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total_sendsize size: {} bytes\".format(total_size))\n",
    "print('\\n')\n",
    "\n",
    "print('---total_receivesize_list---')\n",
    "total_size = 0\n",
    "for size in total_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total receive sizes: {} bytes\".format(total_size) )\n",
    "print('\\n')\n",
    "\n",
    "for i in range(users):\n",
    "    print('---client_sendsize_list(user{})---'.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_sendsize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    print(\"total client_sendsizes(user{}): {} bytes\".format(i, total_size))\n",
    "    print('\\n')\n",
    "\n",
    "    print('---client_receivesize_list(user{})---'.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_receivesize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    print(\"total client_receive sizes(user{}): {} bytes\".format(i, total_size))\n",
    "    print('\\n')\n",
    "\n",
    "print('---train_sendsize_list---')\n",
    "total_size = 0\n",
    "for size in train_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total train_sendsizes: {} bytes\".format(total_size))\n",
    "print('\\n')\n",
    "\n",
    "print('---train_receivesize_list---')\n",
    "total_size = 0\n",
    "for size in train_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total train_receivesizes: {} bytes\".format(total_size))\n",
    "print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
